[scrapyd]
# 项目的eggs存储位置
eggs_dir = /scrapyd/eggs

# scrapy日志的存储目录。如果要禁用存储日志，请将此选项设置为空，logs_dir=
logs_dir = /scrapyd/logs

# scrapyitem将被存储的目录，默认情况下禁用此项，如果设置了值，会覆盖scrapy的FEED_URI配置项
items_dir = 

# 每个蜘蛛保持完成的工作数量。默认为5
jobs_to_keep = 5

# 项目数据库存储的目录
dbs_dir = /scrapyd/dbs

# 并发scrapy进程的最大数量，默认为0，没有设置或者设置为0时，将使用系统中可用的cpus数乘以max_proc_per_cpu配置的值
max_proc = 0

# 每个CPU启动的进程数，默认4
max_proc_per_cpu = 4

# 保留在启动器中的完成进程的数量。默认为100
finished_to_keep = 100

# 用于轮询队列的时间间隔，以秒为单位。默认为5.0
poll_interval = 5.0

# webservices监听地址
bind_address = 0.0.0.0

# 默认 http 监听端口
http_port = 6800

# 是否调试模式
debug = off

# 将用于启动子流程的模块，可以使用自己的模块自定义从Scrapyd启动的Scrapy进程
runner = scrapyd.runner
application = scrapyd.app.application
launcher = scrapyd.launcher.Launcher
webroot = scrapyd.website.Root

[services]
schedule.json     = scrapyd.webservice.Schedule
cancel.json       = scrapyd.webservice.Cancel
addversion.json   = scrapyd.webservice.AddVersion
listprojects.json = scrapyd.webservice.ListProjects
listversions.json = scrapyd.webservice.ListVersions
listspiders.json  = scrapyd.webservice.ListSpiders
delproject.json   = scrapyd.webservice.DeleteProject
delversion.json   = scrapyd.webservice.DeleteVersion
listjobs.json     = scrapyd.webservice.ListJobs
daemonstatus.json = scrapyd.webservice.DaemonStatus
